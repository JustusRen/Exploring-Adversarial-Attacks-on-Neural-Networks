{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n29BmTnLJwG6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import imageio\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10650,
     "status": "ok",
     "timestamp": 1652879183578,
     "user": {
      "displayName": "Justus Renkhoff",
      "userId": "12982392699296254637"
     },
     "user_tz": -120
    },
    "id": "Td2WGoGiKwsh",
    "outputId": "7a53c970-262e-457d-91cc-d7d0eda83937"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "#!pip install keras-resnet\n",
    "from keras_resnet.models import ResNet50\n",
    "from keras.layers import Input\n",
    "# from scipy.misc import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PUxzcWrtJyVW"
   },
   "outputs": [],
   "source": [
    "model_layer_weights_top_k = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "58-VBdFbJ011"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    input_img_data = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    input_img_data = np.expand_dims(input_img_data, axis=0)\n",
    "    input_img_data = preprocess_input(input_img_data)  # final input shape = (1,224,224,3)\n",
    "    return input_img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hvqWmq6IJ2QI"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x = x.reshape((224, 224, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx7ePeUYDAa9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gcc-fyXfJ4_3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def decode_label(pred):\n",
    "    return decode_predictions(pred)[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NOSgKjJ4J6Rj"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "csz3dYzYJ7Wk"
   },
   "outputs": [],
   "source": [
    "def constraint_occl(gradients, start_point, rect_shape):\n",
    "    new_grads = np.zeros_like(gradients)\n",
    "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
    "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
    "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
    "    return new_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u2EwvhRyJ8hQ"
   },
   "outputs": [],
   "source": [
    "def constraint_light(gradients):\n",
    "    new_grads = np.ones_like(gradients)\n",
    "    grad_mean = 1e4 * np.mean(gradients)\n",
    "    return grad_mean * new_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DNPa6IRpJ9zu"
   },
   "outputs": [],
   "source": [
    "\n",
    "def constraint_black(gradients, rect_shape=(10, 10)):\n",
    "    start_point = (\n",
    "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
    "    new_grads = np.zeros_like(gradients)\n",
    "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
    "    if np.mean(patch) < 0:\n",
    "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
    "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
    "    return new_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bk9Wkm5YJ_PZ"
   },
   "outputs": [],
   "source": [
    "def init_coverage_tables(model1, model2, model3):\n",
    "    model_layer_dict1 = defaultdict(bool)\n",
    "    model_layer_dict2 = defaultdict(bool)\n",
    "    model_layer_dict3 = defaultdict(bool)\n",
    "    init_dict(model1, model_layer_dict1)\n",
    "    init_dict(model2, model_layer_dict2)\n",
    "    init_dict(model3, model_layer_dict3)\n",
    "    return model_layer_dict1, model_layer_dict2, model_layer_dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YG1l2XdxKBXO"
   },
   "outputs": [],
   "source": [
    "def init_coverage_tables(model1):\n",
    "    model_layer_dict1 = defaultdict(bool)\n",
    "    init_dict(model1, model_layer_dict1)\n",
    "    return model_layer_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_ec37hxwKCVU"
   },
   "outputs": [],
   "source": [
    "def init_dict(model, model_layer_dict):\n",
    "    for layer in model.layers:\n",
    "        if 'flatten' in layer.name or 'input' in layer.name:\n",
    "            continue\n",
    "        for index in range(layer.output_shape[-1]):\n",
    "            model_layer_dict[(layer.name, index)] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9W3FLX1VKDSO"
   },
   "outputs": [],
   "source": [
    "def init_coverage_times(model):\n",
    "    model_layer_times = defaultdict(int)\n",
    "    init_times(model,model_layer_times)\n",
    "    return model_layer_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Gbm3hVsIKENJ"
   },
   "outputs": [],
   "source": [
    "def init_coverage_value(model):\n",
    "    model_layer_value = defaultdict(float)\n",
    "    init_times(model, model_layer_value)\n",
    "    return model_layer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ciwjFwtsKFE3"
   },
   "outputs": [],
   "source": [
    "def init_times(model,model_layer_times):\n",
    "    for layer in model.layers:\n",
    "        if 'flatten' in layer.name or 'input' in layer.name:\n",
    "            continue\n",
    "        for index in range(layer.output_shape[-1]):\n",
    "            model_layer_times[(layer.name, index)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fuls9G0-KF_H"
   },
   "outputs": [],
   "source": [
    "def neuron_to_cover(model_layer_dict):\n",
    "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
    "    if not_covered:\n",
    "        layer_name, index = random.choice(not_covered)\n",
    "    else:\n",
    "        layer_name, index = random.choice(model_layer_dict.keys())\n",
    "    return layer_name, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uEPoWmEcKHlp"
   },
   "outputs": [],
   "source": [
    "def neuron_to_cover(not_covered,model_layer_dict):\n",
    "    if not_covered:\n",
    "        layer_name, index = random.choice(not_covered)\n",
    "        not_covered.remove((layer_name, index))\n",
    "    else:\n",
    "        layer_name, index = random.choice(model_layer_dict.keys())\n",
    "    return layer_name, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fgLgIsJpKIXf"
   },
   "outputs": [],
   "source": [
    "def random_strategy(model,model_layer_times, neuron_to_cover_num):\n",
    "    loss_neuron = []\n",
    "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_times.items() if v == 0]\n",
    "    for _ in range(neuron_to_cover_num):\n",
    "        layer_name, index = neuron_to_cover(not_covered, model_layer_times)\n",
    "        loss00_neuron = K.mean(model.get_layer(layer_name).output[..., index])\n",
    "        loss_neuron.append(loss00_neuron)\n",
    "    return loss_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Fqp6rISOKJso"
   },
   "outputs": [],
   "source": [
    "def neuron_select_high_weight(model, layer_names, top_k):\n",
    "    global model_layer_weights_top_k\n",
    "    model_layer_weights_dict = {}\n",
    "    for layer_name in layer_names:\n",
    "        weights = model.get_layer(layer_name).get_weights()\n",
    "        if len(weights) <= 0:\n",
    "            continue\n",
    "        w = np.asarray(weights[0])  # 0 is weights, 1 is biases\n",
    "        w = w.reshape(w.shape)\n",
    "        for index in range(model.get_layer(layer_name).output_shape[-1]):\n",
    "            index_w = np.mean(w[..., index])\n",
    "            if index_w <= 0:\n",
    "                continue\n",
    "            model_layer_weights_dict[(layer_name,index)]=index_w\n",
    "    # notice!\n",
    "    model_layer_weights_list = sorted(model_layer_weights_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    k = 0\n",
    "    for (layer_name, index),weight in model_layer_weights_list:\n",
    "        if k >= top_k:\n",
    "            break\n",
    "        model_layer_weights_top_k.append([layer_name,index])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "15qJ8DcaKMKu"
   },
   "outputs": [],
   "source": [
    "def neuron_selection(model, model_layer_times, model_layer_value, neuron_select_strategy, neuron_to_cover_num,threshold):\n",
    "    if neuron_select_strategy == 'None':\n",
    "        return random_strategy(model, model_layer_times, neuron_to_cover_num)\n",
    "\n",
    "    num_strategy = len([x for x in neuron_select_strategy if x in ['0', '1', '2', '3']])\n",
    "\n",
    "    neuron_to_cover_num_each = neuron_to_cover_num / num_strategy\n",
    "\n",
    "    loss_neuron = []\n",
    "    # initialization for strategies\n",
    "    if ('0' in list(neuron_select_strategy)) or ('1' in list(neuron_select_strategy)):\n",
    "        i = 0\n",
    "        neurons_covered_times = []\n",
    "        neurons_key_pos = {}\n",
    "        for (layer_name, index), time in model_layer_times.items():\n",
    "            neurons_covered_times.append(time)\n",
    "            neurons_key_pos[i] = (layer_name, index)\n",
    "            i += 1\n",
    "        neurons_covered_times = np.asarray(neurons_covered_times)\n",
    "        times_total = sum(neurons_covered_times)\n",
    "\n",
    "    # select neurons covered often\n",
    "    if '0' in list(neuron_select_strategy):\n",
    "        if times_total == 0:\n",
    "            return random_strategy(model, model_layer_times, 1)#The beginning of no neurons covered\n",
    "        neurons_covered_percentage = neurons_covered_times / float(times_total)\n",
    "        num_neuron0 = np.random.choice(range(len(neurons_covered_times)), int(neuron_to_cover_num_each), replace=False, p=neurons_covered_percentage)\n",
    "        for num in num_neuron0:\n",
    "            layer_name0, index0 = neurons_key_pos[num]\n",
    "            loss0_neuron = K.mean(model.get_layer(layer_name0).output[..., index0])\n",
    "            loss_neuron.append(loss0_neuron)\n",
    "\n",
    "    # select neurons covered rarely\n",
    "    if '1' in list(neuron_select_strategy):\n",
    "        if times_total == 0:\n",
    "            return random_strategy(model, model_layer_times, 1)\n",
    "        neurons_covered_times_inverse = np.subtract(max(neurons_covered_times), neurons_covered_times)\n",
    "        neurons_covered_percentage_inverse = neurons_covered_times_inverse / float(sum(neurons_covered_times_inverse))\n",
    "        # num_neuron1 = np.random.choice(range(len(neurons_covered_times)), p=neurons_covered_percentage_inverse)\n",
    "        num_neuron1 = np.random.choice(range(len(neurons_covered_times)), int(neuron_to_cover_num_each), replace=False,\n",
    "                                       p=neurons_covered_percentage_inverse)\n",
    "        for num in num_neuron1:\n",
    "            layer_name1, index1 = neurons_key_pos[num]\n",
    "            loss1_neuron = K.mean(model.get_layer(layer_name1).output[..., index1])\n",
    "            loss_neuron.append(loss1_neuron)\n",
    "\n",
    "    # select neurons with largest weights (feature maps with largest filter weights)\n",
    "    if '2' in list(neuron_select_strategy):\n",
    "        layer_names = [layer.name for layer in model.layers if\n",
    "                       'flatten' not in layer.name and 'input' not in layer.name]\n",
    "        k = 0.1\n",
    "        top_k = k * len(model_layer_times)  # number of neurons to be selected within\n",
    "        global model_layer_weights_top_k\n",
    "        if len(model_layer_weights_top_k) == 0:\n",
    "            neuron_select_high_weight(model, layer_names, top_k)  # Set the value\n",
    "        num_neuron2 = np.random.choice(range(len(model_layer_weights_top_k)), int(neuron_to_cover_num_each), replace=False)\n",
    "        for i in num_neuron2:\n",
    "            layer_name2 = model_layer_weights_top_k[i][0]\n",
    "            index2 = model_layer_weights_top_k[i][1]\n",
    "            loss2_neuron = K.mean(model.get_layer(layer_name2).output[..., index2])\n",
    "            loss_neuron.append(loss2_neuron)\n",
    "\n",
    "    if '3' in list(neuron_select_strategy):\n",
    "        above_threshold = []\n",
    "        below_threshold = []\n",
    "        above_num = neuron_to_cover_num_each / 2\n",
    "        below_num = neuron_to_cover_num_each - above_num\n",
    "        above_i = 0\n",
    "        below_i = 0\n",
    "        for (layer_name, index), value in model_layer_value.items():\n",
    "            if threshold + 0.25 > value > threshold and layer_name != 'fc1' and layer_name != 'fc2' and \\\n",
    "                    layer_name != 'predictions' and layer_name != 'fc1000' and above_i < above_num:\n",
    "                above_threshold.append([layer_name, index])\n",
    "                above_i += 1\n",
    "            elif threshold > value > threshold - 0.2 and layer_name != 'fc1' and layer_name != 'fc2' and \\\n",
    "                    layer_name != 'predictions' and layer_name != 'fc1000' and below_i < below_num:\n",
    "                below_threshold.append([layer_name, index])\n",
    "                below_i += 1\n",
    "\n",
    "        loss_neuron = []\n",
    "        if len(above_threshold) > 0:\n",
    "            for above_item in range(len(above_threshold)):\n",
    "                loss_neuron.append(K.mean(\n",
    "                    model.get_layer(above_threshold[above_item][0]).output[..., above_threshold[above_item][1]]))\n",
    "\n",
    "        if len(below_threshold) > 0:\n",
    "            for below_item in range(len(below_threshold)):\n",
    "                loss_neuron.append(-K.mean(\n",
    "                    model.get_layer(below_threshold[below_item][0]).output[..., below_threshold[below_item][1]]))\n",
    "\n",
    "        if loss_neuron == 0:\n",
    "            return random_strategy(model, model_layer_times, 1)  # The beginning of no neurons covered\n",
    "\n",
    "    return loss_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JeXtsao0KNmx"
   },
   "outputs": [],
   "source": [
    "def neuron_scale(loss_neuron):\n",
    "    loss_neuron_new = []\n",
    "    loss_sum = K.sum(loss_neuron)\n",
    "    for loss_each in loss_neuron:\n",
    "        loss_each /= loss_sum\n",
    "        loss_neuron_new.append(loss_each)\n",
    "    return loss_neuron_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "F8fcEqzzKW9-"
   },
   "outputs": [],
   "source": [
    "def neuron_scale_maxmin(loss_neuron):\n",
    "    max_loss = K.max(loss_neuron)\n",
    "    min_loss = K.min(loss_neuron)\n",
    "    base = max_loss - min_loss\n",
    "    loss_neuron_new = []\n",
    "    for loss_each in loss_neuron:\n",
    "        loss_each_new = (loss_each - min_loss) / base\n",
    "        loss_neuron_new.append(loss_each_new)\n",
    "    return loss_neuron_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "r7USR7gKKYGH"
   },
   "outputs": [],
   "source": [
    "def neuron_covered(model_layer_times):\n",
    "    covered_neurons = len([v for v in model_layer_times.values() if v > 0])\n",
    "    total_neurons = len(model_layer_times)\n",
    "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0KX0LhzSKZH_"
   },
   "outputs": [],
   "source": [
    "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
    "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
    "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
    "    X_scaled = X_std * (rmax - rmin) + rmin\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckB4QDDUd305"
   },
   "source": [
    "Instead of NC use and test KMNC, NBC, SNAC and TKNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "b1XRMv3UKagG"
   },
   "outputs": [],
   "source": [
    "def update_coverage(input_data, model, model_layer_times, threshold=0):\n",
    "    layer_names = [layer.name for layer in model.layers if\n",
    "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
    "\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
    "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
    "\n",
    "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
    "        scaled = scale(intermediate_layer_output[0])\n",
    "        # xrange(scaled.shape[-1])\n",
    "        for num_neuron in range(scaled.shape[-1]):\n",
    "            if np.mean(scaled[..., num_neuron]) > threshold: #and model_layer_dict[(layer_names[i], num_neuron)] == 0:\n",
    "                model_layer_times[(layer_names[i], num_neuron)] += 1\n",
    "\n",
    "    return intermediate_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PVdvtYWiKcXs"
   },
   "outputs": [],
   "source": [
    "def update_coverage_value(input_data, model, model_layer_value):\n",
    "    layer_names = [layer.name for layer in model.layers if\n",
    "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
    "\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
    "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
    "\n",
    "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
    "        scaled = scale(intermediate_layer_output[0])\n",
    "        # xrange(scaled.shape[-1])\n",
    "        for num_neuron in range(scaled.shape[-1]):\n",
    "            model_layer_value[(layer_names[i], num_neuron)] = np.mean(scaled[..., num_neuron])\n",
    "\n",
    "    return intermediate_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VVMarpZeKdbX"
   },
   "outputs": [],
   "source": [
    "def full_coverage(model_layer_dict):\n",
    "    if False in model_layer_dict.values():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9DpKjUqRKets"
   },
   "outputs": [],
   "source": [
    "def fired(model, layer_name, index, input_data, threshold=0):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
    "    scaled = scale(intermediate_layer_output)\n",
    "    if np.mean(scaled[..., index]) > threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "t9ophWE8Kf9t"
   },
   "outputs": [],
   "source": [
    "def diverged(predictions1, predictions2, predictions3, target):\n",
    "    if not predictions1 == predictions2 == predictions3:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "M725YwKSi7CU"
   },
   "outputs": [],
   "source": [
    "def get_signature():\n",
    "    now = datetime.now()\n",
    "    past = datetime(2015, 6, 6, 0, 0, 0, 0)\n",
    "    timespan = now - past\n",
    "    time_sig = int(timespan.total_seconds() * 1000)\n",
    "\n",
    "    return str(time_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIE5eGCFLGd_"
   },
   "source": [
    "# main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP-y_IwpLbZP"
   },
   "source": [
    "example input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DHjimDwJLFCt"
   },
   "outputs": [],
   "source": [
    "# [2] 0.25 10 0602 3 vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOEZK06yLhLF"
   },
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iKU7LmxfLkb7"
   },
   "outputs": [],
   "source": [
    "# e.g.[0,1,2] None for neurons not covered, 0 for covered often, 1 for covered rarely, 2 for high weights\n",
    "neuron_select_strategy = '1'\n",
    "threshold = float(0.25)\n",
    "neuron_to_cover_num = int(10)\n",
    "iteration_times = int(1)\n",
    "model_name = 'vgg16'\n",
    "perturb_levels = [0.25, 0.5, 1, 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10227,
     "status": "ok",
     "timestamp": 1652879213665,
     "user": {
      "displayName": "Justus Renkhoff",
      "userId": "12982392699296254637"
     },
     "user_tz": -120
    },
    "id": "GWv3MoKkLpBN",
    "outputId": "cb53f773-b7db-4e89-94fc-23178fcb4d09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justu\\anaconda3\\envs\\dlfuzz_env\\lib\\site-packages\\keras\\backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 224, 224\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "# define input tensor as a placeholder\n",
    "input_tensor = Input(shape=input_shape)\n",
    "\n",
    "# load multiple models sharing same input tensor\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "if model_name == 'vgg16':\n",
    "    model1 = VGG16(input_tensor=input_tensor)\n",
    "elif model_name == 'vgg19':\n",
    "    model1 = VGG19(input_tensor=input_tensor)\n",
    "elif model_name == 'resnet50':\n",
    "    model1 = ResNet50(input_tensor=input_tensor)\n",
    "else:\n",
    "    print('please specify model name')\n",
    "    os._exit(0)\n",
    "\n",
    "print(model1.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 536143,
     "status": "ok",
     "timestamp": 1652879750680,
     "user": {
      "displayName": "Justus Renkhoff",
      "userId": "12982392699296254637"
     },
     "user_tz": -120
    },
    "id": "9qVVy__LiqWx",
    "outputId": "63ce00c1-239f-4ee3-efc7-fc3e717ba378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet/seeds_20/ILSVRC2012_test_00000055.JPEG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justu\\anaconda3\\envs\\dlfuzz_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covered neurons percentage 14888 neurons 0.050\n",
      "used time : 2.7795950000000005\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000227.JPEG\n",
      "covered neurons percentage 14888 neurons 0.109\n",
      "used time : 2.9460136000000006\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000380.JPEG\n",
      "covered neurons percentage 14888 neurons 0.149\n",
      "used time : 3.2137525000000053\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001117.JPEG\n",
      "covered neurons percentage 14888 neurons 0.185\n",
      "used time : 2.8823539000000054\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001909.JPEG\n",
      "covered neurons percentage 14888 neurons 0.216\n",
      "used time : 3.781236100000001\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000055.JPEG\n",
      "covered neurons percentage 14888 neurons 0.050\n",
      "used time : 3.1880957999999993\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000227.JPEG\n",
      "covered neurons percentage 14888 neurons 0.109\n",
      "used time : 3.165190900000006\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000380.JPEG\n",
      "covered neurons percentage 14888 neurons 0.149\n",
      "used time : 3.9852519999999956\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001117.JPEG\n",
      "covered neurons percentage 14888 neurons 0.185\n",
      "used time : 3.465159100000001\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001909.JPEG\n",
      "covered neurons percentage 14888 neurons 0.216\n",
      "used time : 3.8732411000000013\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000055.JPEG\n",
      "covered neurons percentage 14888 neurons 0.050\n",
      "used time : 3.822322900000003\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000227.JPEG\n",
      "covered neurons percentage 14888 neurons 0.109\n",
      "used time : 3.667565600000003\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000380.JPEG\n",
      "covered neurons percentage 14888 neurons 0.149\n",
      "used time : 4.010103699999988\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001117.JPEG\n",
      "covered neurons percentage 14888 neurons 0.185\n",
      "used time : 4.464238699999996\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001909.JPEG\n",
      "covered neurons percentage 14888 neurons 0.216\n",
      "used time : 4.313974400000006\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000055.JPEG\n",
      "covered neurons percentage 14888 neurons 0.050\n",
      "used time : 4.786704400000005\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000227.JPEG\n",
      "covered neurons percentage 14888 neurons 0.126\n",
      "used time : 6.150754000000006\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000380.JPEG\n",
      "covered neurons percentage 14888 neurons 0.165\n",
      "used time : 4.43622529999999\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001117.JPEG\n",
      "covered neurons percentage 14888 neurons 0.205\n",
      "used time : 6.225247499999995\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001909.JPEG\n",
      "covered neurons percentage 14888 neurons 0.238\n",
      "used time : 5.624507700000009\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000055.JPEG\n",
      "covered neurons percentage 14888 neurons 0.056\n",
      "used time : 5.393697000000003\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000227.JPEG\n",
      "covered neurons percentage 14888 neurons 0.130\n",
      "used time : 6.371258999999995\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00000380.JPEG\n",
      "covered neurons percentage 14888 neurons 0.169\n",
      "used time : 5.434142899999998\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001117.JPEG\n",
      "covered neurons percentage 14888 neurons 0.209\n",
      "used time : 6.622089699999989\n",
      "__________________________________\n",
      "ImageNet/seeds_20/ILSVRC2012_test_00001909.JPEG\n",
      "covered neurons percentage 14888 neurons 0.237\n",
      "used time : 5.305085600000012\n",
      "__________________________________\n"
     ]
    }
   ],
   "source": [
    "img_dir = 'ImageNet/seeds_20/'\n",
    "img_paths = os.listdir(img_dir)\n",
    "img_num = len(img_paths)\n",
    "\n",
    "predict_weight = 0.5\n",
    "neuron_to_cover_weight = 0.5\n",
    "learning_step = 0.5\n",
    "for perturb_level in perturb_levels:\n",
    "    \n",
    "    # model_layer_dict1 = init_coverage_tables(model1)\n",
    "    model_layer_times1 = init_coverage_times(model1)  # times of each neuron covered\n",
    "    model_layer_times2 = init_coverage_times(model1)  # update when new image and adversarial images found\n",
    "    model_layer_value1 = init_coverage_value(model1)\n",
    "    \n",
    "    perturb_level_tmp = str(perturb_level)\n",
    "    perturb_level_tmp = perturb_level_tmp.replace(\".\", \"_\")\n",
    "    save_dir = 'ImageNet/generated_inputs/' + \"perturb_level_\" + perturb_level_tmp + '/'\n",
    "\n",
    "    if os.path.exists(save_dir+'perturb/'):\n",
    "        for i in os.listdir(save_dir+'perturb/'):\n",
    "            path_file = os.path.join(save_dir+'perturb/', i)\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\n",
    "    if os.path.exists(save_dir+'gen_img/'):\n",
    "        for i in os.listdir(save_dir+'gen_img/'):\n",
    "            path_file = os.path.join(save_dir+'gen_img/', i)\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\n",
    "    \"\"\"if os.path.exists(save_dir+'random_img/'):\n",
    "        for i in os.listdir(save_dir+'random_img/'):\n",
    "            path_file = os.path.join(save_dir+'random_img/', i)\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\n",
    "    if os.path.exists(save_dir+'random_perturb/'):\n",
    "        for i in os.listdir(save_dir+'random_perturb/'):\n",
    "            path_file = os.path.join(save_dir+'random_perturb/', i)\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\"\"\"\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    if not os.path.exists(save_dir+'perturb/'):\n",
    "        os.makedirs(save_dir+'perturb/')\n",
    "    if not os.path.exists(save_dir+'gen_img/'):\n",
    "        os.makedirs(save_dir+'gen_img/')\n",
    "    \"\"\"if not os.path.exists(save_dir+'random_img/'):\n",
    "        os.makedirs(save_dir+'random_img/')\n",
    "    if not os.path.exists(save_dir+'random_perturb/'):\n",
    "        os.makedirs(save_dir+'random_perturb/')\"\"\"\n",
    "    \n",
    "    # start = time.clock()\n",
    "    total_time = 0\n",
    "    total_norm = 0\n",
    "    adversial_num = 0\n",
    "    total_perturb_adversial = 0\n",
    "    \n",
    "    for i in range(img_num):\n",
    "        start_time = time.perf_counter()\n",
    "        img_list = []\n",
    "        img_path = os.path.join(img_dir,img_paths[i])\n",
    "        print(img_path)\n",
    "        tmp_img = preprocess_image(img_path)\n",
    "        orig_img = tmp_img.copy()\n",
    "        img_list.append(tmp_img)\n",
    "        update_coverage(tmp_img, model1, model_layer_times2, threshold)\n",
    "        while len(img_list) > 0:\n",
    "            gen_img = img_list[0]\n",
    "            img_list.remove(gen_img)\n",
    "\n",
    "            # first check if input already induces differences\n",
    "            pred1 = model1.predict(gen_img)\n",
    "            label1 = np.argmax(pred1[0])\n",
    "\n",
    "            label_top5 = np.argsort(pred1[0])[-5:]\n",
    "\n",
    "            update_coverage_value(gen_img, model1, model_layer_value1)\n",
    "            update_coverage(gen_img, model1, model_layer_times1, threshold)\n",
    "\n",
    "            orig_label = label1\n",
    "            orig_pred = pred1\n",
    "\n",
    "            if model1.name == 'resnet50':\n",
    "                loss_1 = K.mean(model1.get_layer('fc1000').output[..., orig_label])\n",
    "                loss_2 = K.mean(model1.get_layer('fc1000').output[..., label_top5[-2]])\n",
    "                loss_3 = K.mean(model1.get_layer('fc1000').output[..., label_top5[-3]])\n",
    "                loss_4 = K.mean(model1.get_layer('fc1000').output[..., label_top5[-4]])\n",
    "                loss_5 = K.mean(model1.get_layer('fc1000').output[..., label_top5[-5]])\n",
    "\n",
    "            else:\n",
    "                loss_1 = K.mean(model1.get_layer('predictions').output[..., orig_label])\n",
    "                loss_2 = K.mean(model1.get_layer('predictions').output[..., label_top5[-2]])\n",
    "                loss_3 = K.mean(model1.get_layer('predictions').output[..., label_top5[-3]])\n",
    "                loss_4 = K.mean(model1.get_layer('predictions').output[..., label_top5[-4]])\n",
    "                loss_5 = K.mean(model1.get_layer('predictions').output[..., label_top5[-5]])\n",
    "\n",
    "            layer_output = (predict_weight * (loss_2 + loss_3 + loss_4 + loss_5) - loss_1)\n",
    "\n",
    "            # neuron coverage loss\n",
    "            loss_neuron = neuron_selection(model1, model_layer_times1, model_layer_value1, neuron_select_strategy, neuron_to_cover_num, threshold)\n",
    "\n",
    "            # extreme value means the activation value for a neuron can be as high as possible ...\n",
    "            EXTREME_VALUE = False\n",
    "            if EXTREME_VALUE:\n",
    "                neuron_to_cover_weight = 2\n",
    "\n",
    "            layer_output += neuron_to_cover_weight * K.sum(loss_neuron)\n",
    "\n",
    "            # for adversarial image generation\n",
    "            final_loss = K.mean(layer_output)\n",
    "\n",
    "            # we compute the gradient of the input picture wrt this loss\n",
    "            grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
    "\n",
    "            grads_tensor_list = [loss_1, loss_2, loss_3, loss_4, loss_5]\n",
    "            grads_tensor_list.extend(loss_neuron)\n",
    "            grads_tensor_list.append(grads)\n",
    "            # this function returns the loss and grads given the input picture\n",
    "\n",
    "            iterate = K.function([input_tensor], grads_tensor_list)\n",
    "\n",
    "            # we run gradient ascent for some steps\n",
    "            for iters in range(iteration_times):\n",
    "                loss_neuron_list = iterate([gen_img])\n",
    "                perturb = loss_neuron_list[-1] * learning_step * perturb_level\n",
    "\n",
    "                \"\"\"noise = np.random.normal(0, 4, perturb.shape)\n",
    "                # print(noise)\n",
    "                random_perturb = perturb + noise\n",
    "                random_img = gen_img + random_perturb\"\"\"\n",
    "\n",
    "                gen_img += perturb\n",
    "\n",
    "\n",
    "                # previous accumulated neuron coverage\n",
    "                previous_coverage = neuron_covered(model_layer_times1)[2]\n",
    "\n",
    "                pred1 = model1.predict(gen_img)\n",
    "                label1 = np.argmax(pred1[0])\n",
    "\n",
    "                update_coverage(gen_img, model1, model_layer_times1, threshold) # for seed selection\n",
    "                current_coverage = neuron_covered(model_layer_times1)[2]\n",
    "                diff_img = gen_img - orig_img\n",
    "                L2_norm = np.linalg.norm(diff_img)\n",
    "                orig_L2_norm = np.linalg.norm(orig_img)\n",
    "                perturb_adversial = L2_norm / orig_L2_norm\n",
    "\n",
    "                #if current_coverage - previous_coverage > 0.01 / (i + 1) and perturb_adversial < 0.02:\n",
    "                #    img_list.append(gen_img)\n",
    "                    # print('coverage diff = ', current_coverage - previous_coverage, 'perturb_adversial = ', perturb_adversial)\n",
    "\n",
    "                if label1 != orig_label:\n",
    "                    update_coverage(gen_img, model1, model_layer_times2, threshold)\n",
    "                    total_norm += L2_norm\n",
    "                    total_perturb_adversial += perturb_adversial\n",
    "\n",
    "                    # print('L2 norm : ' + str(L2_norm))\n",
    "                    # print('ratio perturb = ', perturb_adversial)\n",
    "\n",
    "                    # get original image id\n",
    "                    _img_path = img_paths[i].split('_')\n",
    "                    _img_path = _img_path[-1]   # 00000xxx.JPEG\n",
    "                    _img_path = _img_path.split('.')\n",
    "                    _img_id = _img_path[0]    # 000000xxx\n",
    "\n",
    "\n",
    "                    perturb_tmp = perturb.copy()\n",
    "                    perturb_deprocessed = deprocess_image(perturb_tmp)\n",
    "                    save_img = save_dir + 'perturb/' + _img_id + '_' +  decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                    imageio.imwrite(save_img, perturb_deprocessed)\n",
    "\n",
    "                    gen_img_tmp = gen_img.copy()\n",
    "                    gen_img_deprocessed = deprocess_image(gen_img_tmp)\n",
    "                    save_img = save_dir + 'gen_img/' + _img_id + '_' + decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                    # imsave(save_img, gen_img_deprocessed)\n",
    "                    imageio.imwrite(save_img, gen_img_deprocessed)\n",
    "\n",
    "                    \"\"\"random_perturb_tmp = random_perturb.copy()\n",
    "                    random_perturb_deprocessed = deprocess_image(random_perturb_tmp)\n",
    "                    save_img = save_dir + 'random_perturb/' + _img_id + '_' +  decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                    imageio.imwrite(save_img, random_perturb_deprocessed)\n",
    "\n",
    "                    random_img_tmp = random_img.copy()\n",
    "                    random_img_deprocessed = deprocess_image(random_img_tmp)\n",
    "                    save_img = save_dir + 'random_img/' + _img_id + '_' + decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                    # imsave(save_img, gen_img_deprocessed)\n",
    "                    imageio.imwrite(save_img, random_img_deprocessed)\"\"\"\n",
    "\n",
    "                    adversial_num += 1\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        print('covered neurons percentage %d neurons %.3f'\n",
    "              % (len(model_layer_times2), neuron_covered(model_layer_times2)[2]))\n",
    "        duration = end_time - start_time\n",
    "        print('used time : ' + str(duration))\n",
    "        total_time += duration\n",
    "        print('__________________________________')\n",
    "    #print('final covered neurons percentage %d neurons %.3f'\n",
    "    #      % (len(model_layer_times2), neuron_covered(model_layer_times2)[2]))\n",
    "    #print('total_time = ' + str(total_time))\n",
    "    #print('average_norm = ' + str(total_norm / adversial_num))\n",
    "    #print('adversial num = ' + str(adversial_num))\n",
    "    #print('average perb adversial = ' + str(total_perturb_adversial / adversial_num))\n",
    "    #print('__________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DLFuzz.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8f1f844e6ef41e6fbe9fde4371280fb1a6eef9f5f890ce22d440d6fa8cc152e2"
  },
  "kernelspec": {
   "display_name": "dlfuzz_env",
   "language": "python",
   "name": "dlfuzz_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
