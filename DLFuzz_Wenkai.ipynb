{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n29BmTnLJwG6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import imageio\n",
    "\n",
    "from keras import backend as K\n",
    "#from numba import cuda\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def release_GPUmemory():\n",
    "    K.clear_session()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10650,
     "status": "ok",
     "timestamp": 1652879183578,
     "user": {
      "displayName": "Justus Renkhoff",
      "userId": "12982392699296254637"
     },
     "user_tz": -120
    },
    "id": "Td2WGoGiKwsh",
    "outputId": "7a53c970-262e-457d-91cc-d7d0eda83937"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "#!pip install keras-resnet\n",
    "from keras_resnet.models import ResNet50\n",
    "from keras.layers import Input\n",
    "# from scipy.misc import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUxzcWrtJyVW"
   },
   "outputs": [],
   "source": [
    "model_layer_weights_top_k = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58-VBdFbJ011"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    input_img_data = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    input_img_data = np.expand_dims(input_img_data, axis=0)\n",
    "    input_img_data = preprocess_input(input_img_data)  # final input shape = (1,224,224,3)\n",
    "    return input_img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvqWmq6IJ2QI"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x = x.reshape((224, 224, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx7ePeUYDAa9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcc-fyXfJ4_3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def decode_label(pred):\n",
    "    return decode_predictions(pred)[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOSgKjJ4J6Rj"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csz3dYzYJ7Wk"
   },
   "outputs": [],
   "source": [
    "def constraint_occl(gradients, start_point, rect_shape):\n",
    "    new_grads = np.zeros_like(gradients)\n",
    "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
    "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
    "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
    "    return new_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2EwvhRyJ8hQ"
   },
   "outputs": [],
   "source": [
    "def constraint_light(gradients):\n",
    "    new_grads = np.ones_like(gradients)\n",
    "    grad_mean = 1e4 * np.mean(gradients)\n",
    "    return grad_mean * new_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNPa6IRpJ9zu"
   },
   "outputs": [],
   "source": [
    "\n",
    "def constraint_black(gradients, rect_shape=(10, 10)):\n",
    "    start_point = (\n",
    "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
    "    new_grads = np.zeros_like(gradients)\n",
    "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
    "    if np.mean(patch) < 0:\n",
    "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
    "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
    "    return new_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bk9Wkm5YJ_PZ"
   },
   "outputs": [],
   "source": [
    "def init_coverage_tables(model1, model2, model3):\n",
    "    model_layer_dict1 = defaultdict(bool)\n",
    "    model_layer_dict2 = defaultdict(bool)\n",
    "    model_layer_dict3 = defaultdict(bool)\n",
    "    init_dict(model1, model_layer_dict1)\n",
    "    init_dict(model2, model_layer_dict2)\n",
    "    init_dict(model3, model_layer_dict3)\n",
    "    return model_layer_dict1, model_layer_dict2, model_layer_dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG1l2XdxKBXO"
   },
   "outputs": [],
   "source": [
    "def init_coverage_tables(model1):\n",
    "    model_layer_dict1 = defaultdict(bool)\n",
    "    init_dict(model1, model_layer_dict1)\n",
    "    return model_layer_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ec37hxwKCVU"
   },
   "outputs": [],
   "source": [
    "def init_dict(model, model_layer_dict):\n",
    "    for layer in model.layers:\n",
    "        if 'flatten' in layer.name or 'input' in layer.name:\n",
    "            continue\n",
    "        for index in range(layer.output_shape[-1]):\n",
    "            model_layer_dict[(layer.name, index)] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W3FLX1VKDSO"
   },
   "outputs": [],
   "source": [
    "def init_coverage_times(model):\n",
    "    model_layer_times = defaultdict(int)\n",
    "    init_times(model,model_layer_times)\n",
    "    return model_layer_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gbm3hVsIKENJ"
   },
   "outputs": [],
   "source": [
    "def init_coverage_value(model):\n",
    "    model_layer_value = defaultdict(float)\n",
    "    init_times(model, model_layer_value)\n",
    "    return model_layer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciwjFwtsKFE3"
   },
   "outputs": [],
   "source": [
    "def init_times(model,model_layer_times):\n",
    "    for layer in model.layers:\n",
    "        if 'flatten' in layer.name or 'input' in layer.name:\n",
    "            continue\n",
    "        for index in range(layer.output_shape[-1]):\n",
    "            model_layer_times[(layer.name, index)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuls9G0-KF_H"
   },
   "outputs": [],
   "source": [
    "def neuron_to_cover(model_layer_dict):\n",
    "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
    "    if not_covered:\n",
    "        layer_name, index = random.choice(not_covered)\n",
    "    else:\n",
    "        layer_name, index = random.choice(model_layer_dict.keys())\n",
    "    return layer_name, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEPoWmEcKHlp"
   },
   "outputs": [],
   "source": [
    "def neuron_to_cover(not_covered,model_layer_dict):\n",
    "    if not_covered:\n",
    "        layer_name, index = random.choice(not_covered)\n",
    "        not_covered.remove((layer_name, index))\n",
    "    else:\n",
    "        layer_name, index = random.choice(model_layer_dict.keys())\n",
    "    return layer_name, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgLgIsJpKIXf"
   },
   "outputs": [],
   "source": [
    "def random_strategy(model,model_layer_times, neuron_to_cover_num):\n",
    "    loss_neuron = []\n",
    "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_times.items() if v == 0]\n",
    "    for _ in range(neuron_to_cover_num):\n",
    "        layer_name, index = neuron_to_cover(not_covered, model_layer_times)\n",
    "        loss00_neuron = K.mean(model.get_layer(layer_name).output[..., index])\n",
    "        loss_neuron.append(loss00_neuron)\n",
    "    return loss_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fqp6rISOKJso"
   },
   "outputs": [],
   "source": [
    "def neuron_select_high_weight(model, layer_names, top_k):\n",
    "    global model_layer_weights_top_k\n",
    "    model_layer_weights_dict = {}\n",
    "    for layer_name in layer_names:\n",
    "        weights = model.get_layer(layer_name).get_weights()\n",
    "        if len(weights) <= 0:\n",
    "            continue\n",
    "        w = np.asarray(weights[0])  # 0 is weights, 1 is biases\n",
    "        w = w.reshape(w.shape)\n",
    "        for index in range(model.get_layer(layer_name).output_shape[-1]):\n",
    "            index_w = np.mean(w[..., index])\n",
    "            if index_w <= 0:\n",
    "                continue\n",
    "            model_layer_weights_dict[(layer_name,index)]=index_w\n",
    "    # notice!\n",
    "    model_layer_weights_list = sorted(model_layer_weights_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    k = 0\n",
    "    for (layer_name, index),weight in model_layer_weights_list:\n",
    "        if k >= top_k:\n",
    "            break\n",
    "        model_layer_weights_top_k.append([layer_name,index])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15qJ8DcaKMKu"
   },
   "outputs": [],
   "source": [
    "def neuron_selection(model, model_layer_times, model_layer_value, neuron_select_strategy, neuron_to_cover_num,threshold):\n",
    "    if neuron_select_strategy == 'None':\n",
    "        return random_strategy(model, model_layer_times, neuron_to_cover_num)\n",
    "\n",
    "    num_strategy = len([x for x in neuron_select_strategy if x in ['0', '1', '2', '3']])\n",
    "\n",
    "    neuron_to_cover_num_each = neuron_to_cover_num / num_strategy\n",
    "\n",
    "    loss_neuron = []\n",
    "    # initialization for strategies\n",
    "    if ('0' in list(neuron_select_strategy)) or ('1' in list(neuron_select_strategy)):\n",
    "        i = 0\n",
    "        neurons_covered_times = []\n",
    "        neurons_key_pos = {}\n",
    "        for (layer_name, index), time in model_layer_times.items():\n",
    "            neurons_covered_times.append(time)\n",
    "            neurons_key_pos[i] = (layer_name, index)\n",
    "            i += 1\n",
    "        neurons_covered_times = np.asarray(neurons_covered_times)\n",
    "        times_total = sum(neurons_covered_times)\n",
    "\n",
    "    # select neurons covered often\n",
    "    if '0' in list(neuron_select_strategy):\n",
    "        if times_total == 0:\n",
    "            return random_strategy(model, model_layer_times, 1)#The beginning of no neurons covered\n",
    "        neurons_covered_percentage = neurons_covered_times / float(times_total)\n",
    "        num_neuron0 = np.random.choice(range(len(neurons_covered_times)), int(neuron_to_cover_num_each), replace=False, p=neurons_covered_percentage)\n",
    "        for num in num_neuron0:\n",
    "            layer_name0, index0 = neurons_key_pos[num]\n",
    "            loss0_neuron = K.mean(model.get_layer(layer_name0).output[..., index0])\n",
    "            loss_neuron.append(loss0_neuron)\n",
    "\n",
    "    # select neurons covered rarely\n",
    "    if '1' in list(neuron_select_strategy):\n",
    "        if times_total == 0:\n",
    "            return random_strategy(model, model_layer_times, 1)\n",
    "        neurons_covered_times_inverse = np.subtract(max(neurons_covered_times), neurons_covered_times)\n",
    "        neurons_covered_percentage_inverse = neurons_covered_times_inverse / float(sum(neurons_covered_times_inverse))\n",
    "        # num_neuron1 = np.random.choice(range(len(neurons_covered_times)), p=neurons_covered_percentage_inverse)\n",
    "        num_neuron1 = np.random.choice(range(len(neurons_covered_times)), int(neuron_to_cover_num_each), replace=False,\n",
    "                                       p=neurons_covered_percentage_inverse)\n",
    "        for num in num_neuron1:\n",
    "            layer_name1, index1 = neurons_key_pos[num]\n",
    "            loss1_neuron = K.mean(model.get_layer(layer_name1).output[..., index1])\n",
    "            loss_neuron.append(loss1_neuron)\n",
    "\n",
    "    # select neurons with largest weights (feature maps with largest filter weights)\n",
    "    if '2' in list(neuron_select_strategy):\n",
    "        layer_names = [layer.name for layer in model.layers if\n",
    "                       'flatten' not in layer.name and 'input' not in layer.name]\n",
    "        k = 0.1\n",
    "        top_k = k * len(model_layer_times)  # number of neurons to be selected within\n",
    "        global model_layer_weights_top_k\n",
    "        if len(model_layer_weights_top_k) == 0:\n",
    "            neuron_select_high_weight(model, layer_names, top_k)  # Set the value\n",
    "        num_neuron2 = np.random.choice(range(len(model_layer_weights_top_k)), int(neuron_to_cover_num_each), replace=False)\n",
    "        for i in num_neuron2:\n",
    "            layer_name2 = model_layer_weights_top_k[i][0]\n",
    "            index2 = model_layer_weights_top_k[i][1]\n",
    "            loss2_neuron = K.mean(model.get_layer(layer_name2).output[..., index2])\n",
    "            loss_neuron.append(loss2_neuron)\n",
    "\n",
    "    if '3' in list(neuron_select_strategy):\n",
    "        above_threshold = []\n",
    "        below_threshold = []\n",
    "        above_num = neuron_to_cover_num_each / 2\n",
    "        below_num = neuron_to_cover_num_each - above_num\n",
    "        above_i = 0\n",
    "        below_i = 0\n",
    "        for (layer_name, index), value in model_layer_value.items():\n",
    "            if threshold + 0.25 > value > threshold and layer_name != 'fc1' and layer_name != 'fc2' and \\\n",
    "                    layer_name != 'predictions' and layer_name != 'fc1000' and above_i < above_num:\n",
    "                above_threshold.append([layer_name, index])\n",
    "                above_i += 1\n",
    "            elif threshold > value > threshold - 0.2 and layer_name != 'fc1' and layer_name != 'fc2' and \\\n",
    "                    layer_name != 'predictions' and layer_name != 'fc1000' and below_i < below_num:\n",
    "                below_threshold.append([layer_name, index])\n",
    "                below_i += 1\n",
    "\n",
    "        loss_neuron = []\n",
    "        if len(above_threshold) > 0:\n",
    "            for above_item in range(len(above_threshold)):\n",
    "                loss_neuron.append(K.mean(\n",
    "                    model.get_layer(above_threshold[above_item][0]).output[..., above_threshold[above_item][1]]))\n",
    "\n",
    "        if len(below_threshold) > 0:\n",
    "            for below_item in range(len(below_threshold)):\n",
    "                loss_neuron.append(-K.mean(\n",
    "                    model.get_layer(below_threshold[below_item][0]).output[..., below_threshold[below_item][1]]))\n",
    "\n",
    "        if loss_neuron == 0:\n",
    "            return random_strategy(model, model_layer_times, 1)  # The beginning of no neurons covered\n",
    "\n",
    "    return loss_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeXtsao0KNmx"
   },
   "outputs": [],
   "source": [
    "def neuron_scale(loss_neuron):\n",
    "    loss_neuron_new = []\n",
    "    loss_sum = K.sum(loss_neuron)\n",
    "    for loss_each in loss_neuron:\n",
    "        loss_each /= loss_sum\n",
    "        loss_neuron_new.append(loss_each)\n",
    "    return loss_neuron_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8fcEqzzKW9-"
   },
   "outputs": [],
   "source": [
    "def neuron_scale_maxmin(loss_neuron):\n",
    "    max_loss = K.max(loss_neuron)\n",
    "    min_loss = K.min(loss_neuron)\n",
    "    base = max_loss - min_loss\n",
    "    loss_neuron_new = []\n",
    "    for loss_each in loss_neuron:\n",
    "        loss_each_new = (loss_each - min_loss) / base\n",
    "        loss_neuron_new.append(loss_each_new)\n",
    "    return loss_neuron_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7USR7gKKYGH"
   },
   "outputs": [],
   "source": [
    "def neuron_covered(model_layer_times):\n",
    "    covered_neurons = len([v for v in model_layer_times.values() if v > 0])\n",
    "    total_neurons = len(model_layer_times)\n",
    "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KX0LhzSKZH_"
   },
   "outputs": [],
   "source": [
    "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
    "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
    "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
    "    X_scaled = X_std * (rmax - rmin) + rmin\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckB4QDDUd305"
   },
   "source": [
    "Instead of NC use and test KMNC, NBC, SNAC and TKNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1XRMv3UKagG"
   },
   "outputs": [],
   "source": [
    "def update_coverage(input_data, model, model_layer_times, threshold=0):\n",
    "    layer_names = [layer.name for layer in model.layers if\n",
    "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
    "\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
    "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
    "\n",
    "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
    "        scaled = scale(intermediate_layer_output[0])\n",
    "        # xrange(scaled.shape[-1])\n",
    "        for num_neuron in range(scaled.shape[-1]):\n",
    "            if np.mean(scaled[..., num_neuron]) > threshold: #and model_layer_dict[(layer_names[i], num_neuron)] == 0:\n",
    "                model_layer_times[(layer_names[i], num_neuron)] += 1\n",
    "\n",
    "    return intermediate_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVdvtYWiKcXs"
   },
   "outputs": [],
   "source": [
    "def update_coverage_value(input_data, model, model_layer_value):\n",
    "    layer_names = [layer.name for layer in model.layers if\n",
    "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
    "\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
    "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
    "\n",
    "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
    "        scaled = scale(intermediate_layer_output[0])\n",
    "        # xrange(scaled.shape[-1])\n",
    "        for num_neuron in range(scaled.shape[-1]):\n",
    "            model_layer_value[(layer_names[i], num_neuron)] = np.mean(scaled[..., num_neuron])\n",
    "\n",
    "    return intermediate_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVMarpZeKdbX"
   },
   "outputs": [],
   "source": [
    "def full_coverage(model_layer_dict):\n",
    "    if False in model_layer_dict.values():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DpKjUqRKets"
   },
   "outputs": [],
   "source": [
    "def fired(model, layer_name, index, input_data, threshold=0):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
    "    scaled = scale(intermediate_layer_output)\n",
    "    if np.mean(scaled[..., index]) > threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9ophWE8Kf9t"
   },
   "outputs": [],
   "source": [
    "def diverged(predictions1, predictions2, predictions3, target):\n",
    "    if not predictions1 == predictions2 == predictions3:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M725YwKSi7CU"
   },
   "outputs": [],
   "source": [
    "def get_signature():\n",
    "    now = datetime.now()\n",
    "    past = datetime(2015, 6, 6, 0, 0, 0, 0)\n",
    "    timespan = now - past\n",
    "    time_sig = int(timespan.total_seconds() * 1000)\n",
    "\n",
    "    return str(time_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIE5eGCFLGd_"
   },
   "source": [
    "# main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP-y_IwpLbZP"
   },
   "source": [
    "example input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHjimDwJLFCt"
   },
   "outputs": [],
   "source": [
    "# [2] 0.25 10 0602 3 vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOEZK06yLhLF"
   },
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKU7LmxfLkb7"
   },
   "outputs": [],
   "source": [
    "# e.g.[0,1,2] None for neurons not covered, 0 for covered often, 1 for covered rarely, 2 for high weights\n",
    "neuron_select_strategy = '1'\n",
    "threshold = float(0.25)\n",
    "neuron_to_cover_num = int(10)\n",
    "iteration_times = int(1)\n",
    "model_name = 'vgg16'\n",
    "perturb_levels = [0.25, 0.5, 1, 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10227,
     "status": "ok",
     "timestamp": 1652879213665,
     "user": {
      "displayName": "Justus Renkhoff",
      "userId": "12982392699296254637"
     },
     "user_tz": -120
    },
    "id": "GWv3MoKkLpBN",
    "outputId": "cb53f773-b7db-4e89-94fc-23178fcb4d09"
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 224, 224\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "# define input tensor as a placeholder\n",
    "input_tensor = Input(shape=input_shape)\n",
    "\n",
    "# load multiple models sharing same input tensor\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "if model_name == 'vgg16':\n",
    "    model1 = VGG16(input_tensor=input_tensor)\n",
    "elif model_name == 'vgg19':\n",
    "    model1 = VGG19(input_tensor=input_tensor)\n",
    "elif model_name == 'resnet50':\n",
    "    model1 = ResNet50(input_tensor=input_tensor)\n",
    "else:\n",
    "    print('please specify model name')\n",
    "    os._exit(0)\n",
    "\n",
    "print(model1.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 536143,
     "status": "ok",
     "timestamp": 1652879750680,
     "user": {
      "displayName": "Justus Renkhoff",
      "userId": "12982392699296254637"
     },
     "user_tz": -120
    },
    "id": "9qVVy__LiqWx",
    "outputId": "63ce00c1-239f-4ee3-efc7-fc3e717ba378"
   },
   "outputs": [],
   "source": [
    "img_dir = 'ImageNet/seeds_20/'\n",
    "img_paths = os.listdir(img_dir)\n",
    "img_num = len(img_paths)\n",
    "\n",
    "predict_weight = 0.5\n",
    "neuron_to_cover_weight = 0.5\n",
    "learning_step = 0.5\n",
    "\n",
    "cov_dict = {}\n",
    "\n",
    "#for perturb_level in perturb_levels:\n",
    "coverage = []\n",
    "\n",
    "# model_layer_dict1 = init_coverage_tables(model1)\n",
    "model_layer_times1 = init_coverage_times(model1)  # times of each neuron covered\n",
    "model_layer_times2 = init_coverage_times(model1)  # update when new image and adversarial images found\n",
    "model_layer_value1 = init_coverage_value(model1)\n",
    "\n",
    "for perturb_level in perturb_levels:\n",
    "    perturb_level_tmp = str(perturb_level)\n",
    "    perturb_level_tmp = perturb_level_tmp.replace(\".\", \"_\")\n",
    "    save_dir = 'ImageNet/generated_inputs/' + \"perturb_level_\" + perturb_level_tmp + '/'\n",
    "\n",
    "    if os.path.exists(save_dir+'perturb/'):\n",
    "        for i in os.listdir(save_dir+'perturb/'):\n",
    "            path_file = os.path.join(save_dir+'perturb/', i)\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\n",
    "    if os.path.exists(save_dir+'gen_img/'):\n",
    "        for i in os.listdir(save_dir+'gen_img/'):\n",
    "            path_file = os.path.join(save_dir+'gen_img/', i)\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\n",
    "    if os.path.exists(save_dir+'random_img/'):\n",
    "        for i in os.listdir(save_dir+'random_img/'):\n",
    "            path_file = os.path.join(save_dir+'random_img/', i)\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\n",
    "    if os.path.exists(save_dir+'random_perturb/'):\n",
    "        for i in os.listdir(save_dir+'random_perturb/'):\n",
    "            path_file = os.path.join(save_dir+'random_perturb/', i)\n",
    "            if os.path.isfile(path_file):\n",
    "                os.remove(path_file)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    if not os.path.exists(save_dir+'perturb/'):\n",
    "        os.makedirs(save_dir+'perturb/')\n",
    "    if not os.path.exists(save_dir+'gen_img/'):\n",
    "        os.makedirs(save_dir+'gen_img/')\n",
    "    if not os.path.exists(save_dir+'random_img/'):\n",
    "        os.makedirs(save_dir+'random_img/')\n",
    "    if not os.path.exists(save_dir+'random_perturb/'):\n",
    "        os.makedirs(save_dir+'random_perturb/')\n",
    "\n",
    "# start = time.clock()\n",
    "total_time = 0\n",
    "total_norm = 0\n",
    "adversial_num = 0\n",
    "total_perturb_adversial = 0\n",
    "\n",
    "for i in range(img_num):\n",
    "    start_time = time.perf_counter()\n",
    "    img_list = []\n",
    "    img_path = os.path.join(img_dir,img_paths[i])\n",
    "    print(img_path)\n",
    "    tmp_img = preprocess_image(img_path)\n",
    "    orig_img = tmp_img.copy()\n",
    "    img_list.append(tmp_img)\n",
    "    update_coverage(tmp_img, model1, model_layer_times2, threshold)\n",
    "    while len(img_list) > 0:\n",
    "        gen_img = img_list[0]\n",
    "        img_list.remove(gen_img)\n",
    "\n",
    "        # first check if input already induces differences\n",
    "        pred1 = model1.predict(gen_img)\n",
    "        label1 = np.argmax(pred1[0])\n",
    "\n",
    "        label_top5 = np.argsort(pred1[0])[-5:]\n",
    "\n",
    "        update_coverage_value(gen_img, model1, model_layer_value1)\n",
    "        update_coverage(gen_img, model1, model_layer_times1, threshold)\n",
    "\n",
    "        orig_label = label1\n",
    "        orig_pred = pred1\n",
    "\n",
    "        if model1.name == 'resnet50':\n",
    "            loss_1 = K.mean(model1.get_layer('fc1000').output[..., orig_label])\n",
    "            loss_2 = K.mean(model1.get_layer('fc1000').output[..., label_top5[-2]])\n",
    "            loss_3 = K.mean(model1.get_layer('fc1000').output[..., label_top5[-3]])\n",
    "            loss_4 = K.mean(model1.get_layer('fc1000').output[..., label_top5[-4]])\n",
    "            loss_5 = K.mean(model1.get_layer('fc1000').output[..., label_top5[-5]])\n",
    "\n",
    "        else:\n",
    "            loss_1 = K.mean(model1.get_layer('predictions').output[..., orig_label])\n",
    "            loss_2 = K.mean(model1.get_layer('predictions').output[..., label_top5[-2]])\n",
    "            loss_3 = K.mean(model1.get_layer('predictions').output[..., label_top5[-3]])\n",
    "            loss_4 = K.mean(model1.get_layer('predictions').output[..., label_top5[-4]])\n",
    "            loss_5 = K.mean(model1.get_layer('predictions').output[..., label_top5[-5]])\n",
    "\n",
    "        layer_output = (predict_weight * (loss_2 + loss_3 + loss_4 + loss_5) - loss_1)\n",
    "\n",
    "        # neuron coverage loss\n",
    "        loss_neuron = neuron_selection(model1, model_layer_times1, model_layer_value1, neuron_select_strategy, neuron_to_cover_num, threshold)\n",
    "\n",
    "        # extreme value means the activation value for a neuron can be as high as possible ...\n",
    "        EXTREME_VALUE = False\n",
    "        if EXTREME_VALUE:\n",
    "            neuron_to_cover_weight = 2\n",
    "\n",
    "        layer_output += neuron_to_cover_weight * K.sum(loss_neuron)\n",
    "\n",
    "        # for adversarial image generation\n",
    "        final_loss = K.mean(layer_output)\n",
    "\n",
    "        # we compute the gradient of the input picture wrt this loss\n",
    "        grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
    "\n",
    "        grads_tensor_list = [loss_1, loss_2, loss_3, loss_4, loss_5]\n",
    "        grads_tensor_list.extend(loss_neuron)\n",
    "        grads_tensor_list.append(grads)\n",
    "        # this function returns the loss and grads given the input picture\n",
    "\n",
    "        iterate = K.function([input_tensor], grads_tensor_list)\n",
    "\n",
    "        # we run gradient ascent for some steps\n",
    "        for iters in range(iteration_times):\n",
    "            loss_neuron_list = iterate([gen_img])\n",
    "            perturb = loss_neuron_list[-1] * learning_step # * perturb_level\n",
    "\n",
    "            # get random noise\n",
    "            mean = np.mean(perturb)\n",
    "            std = np.std(perturb)\n",
    "            \n",
    "            print(mean)\n",
    "            print(std)\n",
    "            print(perturb.shape)\n",
    "            \n",
    "            random_perturb = np.random.normal(mean, std, perturb.shape)\n",
    "            \n",
    "            print(random_perturb)\n",
    "            \n",
    "            gen_img += perturb\n",
    "            \n",
    "            print(gen_img.shape)\n",
    "\n",
    "            # previous accumulated neuron coverage\n",
    "            previous_coverage = neuron_covered(model_layer_times1)[2]\n",
    "\n",
    "            pred1 = model1.predict(gen_img)\n",
    "            label1 = np.argmax(pred1[0])\n",
    "\n",
    "            update_coverage(gen_img, model1, model_layer_times1, threshold) # for seed selection\n",
    "            current_coverage = neuron_covered(model_layer_times1)[2]\n",
    "            diff_img = gen_img - orig_img\n",
    "            L2_norm = np.linalg.norm(diff_img)\n",
    "            orig_L2_norm = np.linalg.norm(orig_img)\n",
    "            perturb_adversial = L2_norm / orig_L2_norm\n",
    "\n",
    "            if current_coverage - previous_coverage > 0.01 / (i + 1) and perturb_adversial < 0.02:\n",
    "                img_list.append(gen_img)\n",
    "                # print('coverage diff = ', current_coverage - previous_coverage, 'perturb_adversial = ', perturb_adversial)\n",
    "\n",
    "            if label1 != orig_label:\n",
    "                update_coverage(gen_img, model1, model_layer_times2, threshold)\n",
    "                total_norm += L2_norm\n",
    "                total_perturb_adversial += perturb_adversial\n",
    "\n",
    "                # print('L2 norm : ' + str(L2_norm))\n",
    "                # print('ratio perturb = ', perturb_adversial)\n",
    "\n",
    "                # get original image id\n",
    "                _img_path = img_paths[i].split('_')\n",
    "                _img_path = _img_path[-1]   # 00000xxx.JPEG\n",
    "                _img_path = _img_path.split('.')\n",
    "                _img_id = _img_path[0]    # 000000xxx\n",
    "\n",
    "                for perturb_level in perturb_levels:\n",
    "                    gen_img_ori = gen_img - perturb\n",
    "                    gen_img = gen_img_ori + (perturb * perturb_level)\n",
    "\n",
    "                    # get random_img\n",
    "                    random_img = gen_img_ori + (random_perturb * perturb_level)\n",
    "                    \n",
    "                    perturb_level_tmp = str(perturb_level)\n",
    "                    perturb_level_tmp = perturb_level_tmp.replace(\".\", \"_\")\n",
    "                    save_dir = 'ImageNet/generated_inputs/' + \"perturb_level_\" + perturb_level_tmp + '/'\n",
    "\n",
    "                    # save gen_img and perturb\n",
    "                    perturb_tmp = perturb.copy()\n",
    "                    perturb_deprocessed = deprocess_image(perturb_tmp)\n",
    "                    save_img = save_dir + 'perturb/' + _img_id + '_' +  decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                    imageio.imwrite(save_img, perturb_deprocessed)\n",
    "\n",
    "                    gen_img_tmp = gen_img.copy()\n",
    "                    gen_img_deprocessed = deprocess_image(gen_img_tmp)\n",
    "                    save_img = save_dir + 'gen_img/' + _img_id + '_' + decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                    # imsave(save_img, gen_img_deprocessed)\n",
    "                    imageio.imwrite(save_img, gen_img_deprocessed)\n",
    "\n",
    "                    # save random_img and random_perturb\n",
    "                    random_perturb_tmp = random_perturb.copy()\n",
    "                    random_perturb_deprocessed = deprocess_image(random_perturb_tmp)\n",
    "                    save_img = save_dir + 'random_perturb/' + _img_id + '_' + decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                    imageio.imwrite(save_img, random_perturb_deprocessed)\n",
    "\n",
    "                    random_img_tmp = random_img.copy()\n",
    "                    random_img_deprocessed = deprocess_image(random_img_tmp)\n",
    "                    save_img = save_dir + 'random_img/' + _img_id + '_' + decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature())+ '.png'\n",
    "                    imageio.imwrite(save_img, random_img_deprocessed)\n",
    "\n",
    "                \"\"\"random_perturb_tmp = random_perturb.copy()\n",
    "                random_perturb_deprocessed = deprocess_image(random_perturb_tmp)\n",
    "                save_img = save_dir + 'random_perturb/' + _img_id + '_' +  decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                imageio.imwrite(save_img, random_perturb_deprocessed)\n",
    "\n",
    "                random_img_tmp = random_img.copy()\n",
    "                random_img_deprocessed = deprocess_image(random_img_tmp)\n",
    "                save_img = save_dir + 'random_img/' + _img_id + '_' + decode_label(pred1) + '_' + decode_label(orig_pred) + '_' + str(get_signature()) + '.png'\n",
    "                # imsave(save_img, gen_img_deprocessed)\n",
    "                imageio.imwrite(save_img, random_img_deprocessed)\"\"\"\n",
    "\n",
    "                adversial_num += 1\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    print('covered neurons percentage %d neurons %.3f' % (len(model_layer_times2), neuron_covered(model_layer_times2)[2]))\n",
    "    duration = end_time - start_time\n",
    "    print('used time : ' + str(duration))\n",
    "    total_time += duration\n",
    "    print('__________________________________')\n",
    "    #coverage.append(neuron_covered(model_layer_times2)[2])\n",
    "#cov_dict[str(perturb_level)] = coverage  \n",
    "\n",
    "print('final covered neurons percentage %d neurons %.3f' % (len(model_layer_times2), neuron_covered(model_layer_times2)[2]))\n",
    "print('total_time = ' + str(total_time))\n",
    "print('average_norm = ' + str(total_norm / adversial_num))\n",
    "print('adversial num = ' + str(adversial_num))\n",
    "print('average perb adversial = ' + str(total_perturb_adversial / adversial_num))\n",
    "print('__________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cov_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plt.title(\"Coverage for each Perturbation Factor (Normal Perturbation)\")\n",
    "plt.plot(cov_dict[\"0.25\"], label=\"0.25\")\n",
    "plt.plot(cov_dict[\"0.5\"], label=\"0.5\")\n",
    "plt.plot(cov_dict[\"1\"], label=\"1\")\n",
    "plt.plot(cov_dict[\"2\"], label=\"2\")\n",
    "plt.plot(cov_dict[\"4\"], label=\"4\")\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import json\n",
    "with open('dlfuzz_normal.json', 'w') as fp:\n",
    "    json.dump(cov_dict, fp)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release_GPUmemory()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DLFuzz.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dlfuzz_env",
   "language": "python",
   "name": "dlfuzz_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
