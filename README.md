# Exploring-Adversarial-Attacks-on-Neural-Networks
Using the code in this repository, adversarial examples can be generated using DLFuzz [1]. Using Grad-CAM [2], these samples are analyzed and structural vulnerabilities within the architecture of a deep neural network (DNN) can be located. 
<br />
The code from DLFuzz originated in the following repository: https://github.com/turned2670/DLFuzz
<br />
### References
[1] Guo, Jianmin, et al. "Dlfuzz: Differential fuzzing testing of deep learning systems." Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2018.<br />
[2] Selvaraju, Ramprasaath R., et al. "Grad-cam: Visual explanations from deep networks via gradient-based localization." Proceedings of the IEEE international conference on computer vision. 2017.
